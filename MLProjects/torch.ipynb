{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'DESCR', 'feature_names', 'data_filename', 'target_filename', 'data_module'])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(data.data).float()\n",
    "y = torch.tensor(data.target).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.unsqueeze(y, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([442, 1])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([353, 10]), torch.Size([353, 1]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0453, -0.0446, -0.0256,  ..., -0.0395, -0.0320, -0.0756],\n",
       "        [-0.0055,  0.0507, -0.0159,  ...,  0.0343, -0.0181,  0.0445],\n",
       "        [-0.0455,  0.0507,  0.0639,  ...,  0.1081,  0.0757,  0.0859],\n",
       "        ...,\n",
       "        [-0.0055,  0.0507, -0.0418,  ..., -0.0395,  0.0102, -0.0094],\n",
       "        [-0.0564, -0.0446, -0.0741,  ..., -0.0764, -0.0612, -0.0466],\n",
       "        [ 0.0671, -0.0446,  0.0035,  ..., -0.0395, -0.0006,  0.0196]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.connected1 = nn.Linear(in_features = 10, out_features = 12)\n",
    "        self.connected2 = nn.Linear(in_features= 12,  out_features = 8)\n",
    "        self.output = nn.Linear(in_features=8, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.connected1(x))\n",
    "        x = F.relu(self.connected2(x))\n",
    "        x = self.output(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('connected1.weight',\n",
       "              tensor([[-0.2226,  0.2997, -0.3045, -0.2525, -0.0028,  0.0757, -0.1262,  0.0905,\n",
       "                       -0.1029,  0.1253],\n",
       "                      [-0.0492,  0.2964, -0.1627, -0.0914,  0.0563,  0.0280,  0.1693, -0.1459,\n",
       "                        0.2376, -0.1375],\n",
       "                      [-0.2970,  0.2786, -0.0665, -0.1824,  0.2159, -0.0530,  0.2098,  0.2293,\n",
       "                        0.0014, -0.2140],\n",
       "                      [-0.2883, -0.0674, -0.1619, -0.1881, -0.1173,  0.2945,  0.0296,  0.0489,\n",
       "                        0.1571,  0.1759],\n",
       "                      [-0.0226, -0.2304, -0.0393,  0.2068, -0.2410,  0.1563, -0.3027, -0.0070,\n",
       "                        0.0882,  0.0415],\n",
       "                      [ 0.1897,  0.2657, -0.1262,  0.2514, -0.1339, -0.2243,  0.0086,  0.1129,\n",
       "                       -0.2395, -0.2917],\n",
       "                      [ 0.2190,  0.0255, -0.3010,  0.2620, -0.0853,  0.1901, -0.3079,  0.0607,\n",
       "                        0.2788, -0.1414],\n",
       "                      [ 0.0014,  0.2362,  0.2534, -0.0211, -0.1664,  0.3057,  0.1591, -0.0594,\n",
       "                        0.1179, -0.0205],\n",
       "                      [-0.2815,  0.2184,  0.1353, -0.1502,  0.0117,  0.0897,  0.3124, -0.0535,\n",
       "                       -0.0944,  0.1453],\n",
       "                      [-0.1000, -0.2557, -0.2171,  0.0037,  0.3052,  0.1608, -0.2003, -0.1708,\n",
       "                        0.2045,  0.2836],\n",
       "                      [-0.0287, -0.1039,  0.1338,  0.2431, -0.2139, -0.0906, -0.1425, -0.1893,\n",
       "                        0.2036,  0.1177],\n",
       "                      [-0.1997, -0.1027,  0.0098, -0.2759, -0.0326,  0.2600,  0.2048,  0.0738,\n",
       "                       -0.3148, -0.1793]])),\n",
       "             ('connected1.bias',\n",
       "              tensor([-0.1501,  0.0926,  0.1366, -0.0847,  0.2380,  0.1555, -0.2494, -0.2792,\n",
       "                      -0.1326, -0.0131,  0.0048, -0.3074])),\n",
       "             ('connected2.weight',\n",
       "              tensor([[-0.0732, -0.2545, -0.0038,  0.2760,  0.1664, -0.0686, -0.1355,  0.1449,\n",
       "                       -0.2811, -0.0450,  0.0347, -0.1766],\n",
       "                      [-0.1783,  0.2503,  0.0939, -0.0321,  0.1010,  0.1476, -0.2336, -0.0580,\n",
       "                       -0.2193, -0.1329, -0.2583,  0.1822],\n",
       "                      [ 0.0218, -0.1911,  0.2156,  0.1356,  0.2032, -0.1354, -0.0752,  0.0484,\n",
       "                       -0.0261, -0.1390, -0.2262,  0.1567],\n",
       "                      [-0.0998, -0.2582, -0.1783,  0.1715,  0.1105, -0.2200,  0.1558,  0.1446,\n",
       "                       -0.1245, -0.0985,  0.1278,  0.2560],\n",
       "                      [ 0.0179,  0.1954,  0.0547,  0.1950,  0.0717,  0.1252,  0.2344,  0.2100,\n",
       "                        0.0941, -0.2165,  0.1213,  0.1595],\n",
       "                      [-0.1169, -0.1206,  0.1548, -0.2394,  0.0276, -0.2694,  0.2294, -0.0370,\n",
       "                       -0.0762, -0.0881, -0.1961, -0.1315],\n",
       "                      [ 0.1875,  0.1948,  0.2309, -0.0553,  0.0131,  0.2351,  0.0821, -0.0976,\n",
       "                        0.0365, -0.1964,  0.0257,  0.0326],\n",
       "                      [-0.0981,  0.1250, -0.0325, -0.2302,  0.1906, -0.1225,  0.0796,  0.0355,\n",
       "                        0.2120, -0.0772, -0.1992, -0.1954]])),\n",
       "             ('connected2.bias',\n",
       "              tensor([-0.2874,  0.1092, -0.1359,  0.2066, -0.1772,  0.1506,  0.1845, -0.2641])),\n",
       "             ('output.weight',\n",
       "              tensor([[-0.2363, -0.2111, -0.1176, -0.1287, -0.3266, -0.1424,  0.2119, -0.0845]])),\n",
       "             ('output.bias', tensor([-0.2556]))])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegressionModel()\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.forward(X_train)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/rish/Desktop/Mini-Projects/MLProjects/torch.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rish/Desktop/Mini-Projects/MLProjects/torch.ipynb#ch0000009?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rish/Desktop/Mini-Projects/MLProjects/torch.ipynb#ch0000009?line=6'>7</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(X_train)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rish/Desktop/Mini-Projects/MLProjects/torch.ipynb#ch0000009?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_pred, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rish/Desktop/Mini-Projects/MLProjects/torch.ipynb#ch0000009?line=10'>11</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rish/Desktop/Mini-Projects/MLProjects/torch.ipynb#ch0000009?line=12'>13</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:96\u001b[0m, in \u001b[0;36mL1Loss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49ml1_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:3220\u001b[0m, in \u001b[0;36ml1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3216\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, target):\n\u001b[1;32m   3217\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3218\u001b[0m         l1_loss, (\u001b[39minput\u001b[39m, target), \u001b[39minput\u001b[39m, target, size_average\u001b[39m=\u001b[39msize_average, reduce\u001b[39m=\u001b[39mreduce, reduction\u001b[39m=\u001b[39mreduction\n\u001b[1;32m   3219\u001b[0m     )\n\u001b[0;32m-> 3220\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msize()):\n\u001b[1;32m   3221\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   3222\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m). \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3223\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3224\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()),\n\u001b[1;32m   3225\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   3226\u001b[0m     )\n\u001b[1;32m   3227\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    y_pred = model.forward(X_train)\n",
    "\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    ## testing\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model(X_test)\n",
    "\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31b54185ef9236271117b65ccd40447ed8c429f8dbf9fa6a895ee5e4cb482fd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
