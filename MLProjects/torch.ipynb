{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'DESCR', 'feature_names', 'data_filename', 'target_filename', 'data_module'])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(data.data)\n",
    "y = torch.tensor(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.unsqueeze(y, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([442, 1])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.connected1 = nn.Linear(in_features = 10, out_features = 12)\n",
    "        self.connected2 = nn.Linear(in_features= 12,  out_features = 8)\n",
    "        self.output = nn.Linear(in_features=8, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.connected1(x))\n",
    "        x = F.relu(self.connected2(x))\n",
    "        x = self.output(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('connected1.weight',\n",
       "              tensor([[-0.0790,  0.2276, -0.0659,  0.2174,  0.1920, -0.0832,  0.2292,  0.0254,\n",
       "                        0.0965,  0.3063],\n",
       "                      [-0.0325, -0.1448,  0.1883,  0.2881,  0.1885,  0.1051, -0.0451,  0.0028,\n",
       "                        0.2651, -0.1006],\n",
       "                      [-0.1795,  0.1951,  0.1889, -0.1657,  0.1632,  0.1894, -0.0534, -0.2677,\n",
       "                        0.3055,  0.0881],\n",
       "                      [ 0.2028, -0.1673, -0.0297,  0.0283, -0.2623, -0.1576,  0.0670, -0.2727,\n",
       "                        0.2646, -0.0353],\n",
       "                      [-0.2326, -0.2605,  0.1767,  0.2700,  0.2538, -0.0520, -0.0289,  0.2039,\n",
       "                       -0.3091,  0.1754],\n",
       "                      [ 0.1126,  0.3023,  0.0586,  0.2456,  0.1062, -0.2828, -0.2190,  0.0813,\n",
       "                        0.3114,  0.1204],\n",
       "                      [ 0.1440,  0.0049, -0.3088, -0.1528, -0.1483,  0.1486,  0.0181,  0.1129,\n",
       "                       -0.0522,  0.0576],\n",
       "                      [-0.3113, -0.3137,  0.0042,  0.1543,  0.2412, -0.0498, -0.1002, -0.0201,\n",
       "                        0.1550,  0.1774],\n",
       "                      [ 0.1281,  0.0747, -0.1833, -0.3144,  0.0367, -0.2118, -0.0127,  0.2880,\n",
       "                        0.2101,  0.0141],\n",
       "                      [-0.2988,  0.0287, -0.0526,  0.3143, -0.2539, -0.1426, -0.1560,  0.0483,\n",
       "                        0.2277, -0.2088],\n",
       "                      [ 0.0750,  0.0220, -0.0899,  0.2619,  0.0931, -0.3151, -0.1905,  0.1722,\n",
       "                       -0.1245,  0.2178],\n",
       "                      [ 0.2413,  0.0196,  0.2922, -0.2850,  0.0018,  0.0740, -0.1355,  0.1371,\n",
       "                        0.1156, -0.2612]])),\n",
       "             ('connected1.bias',\n",
       "              tensor([-0.3138, -0.0872,  0.1944,  0.2572,  0.0506, -0.2772, -0.2098,  0.2099,\n",
       "                      -0.0988,  0.0781, -0.2845,  0.0903])),\n",
       "             ('connected2.weight',\n",
       "              tensor([[ 0.0554, -0.2431,  0.1052, -0.1475,  0.2712, -0.1949, -0.1959, -0.1782,\n",
       "                       -0.0324,  0.1486, -0.0657, -0.1630],\n",
       "                      [ 0.2307, -0.0928,  0.2824, -0.2422,  0.2497, -0.1940,  0.0012,  0.1877,\n",
       "                        0.1608,  0.1612,  0.0370,  0.1455],\n",
       "                      [-0.1418,  0.2559,  0.1701,  0.2073, -0.1809, -0.1828, -0.2086, -0.1341,\n",
       "                        0.0809, -0.1763,  0.1645,  0.1071],\n",
       "                      [-0.0476, -0.1053,  0.0831,  0.1211, -0.1355,  0.0055, -0.2724,  0.1375,\n",
       "                        0.1415,  0.2302, -0.2257,  0.0621],\n",
       "                      [-0.2790,  0.0574, -0.1805, -0.2804,  0.2308,  0.1065,  0.0677, -0.0445,\n",
       "                        0.2300,  0.0038,  0.2311, -0.1223],\n",
       "                      [ 0.1945,  0.2423,  0.0348, -0.1390,  0.1694,  0.1953, -0.2358,  0.2845,\n",
       "                       -0.2075,  0.1744, -0.1947, -0.0380],\n",
       "                      [ 0.1185, -0.1947, -0.1775, -0.2451, -0.0693,  0.0695,  0.0585, -0.1077,\n",
       "                        0.1848, -0.0206,  0.2105,  0.0712],\n",
       "                      [-0.1323,  0.2463, -0.1836,  0.0778, -0.0346,  0.1727,  0.1348,  0.1510,\n",
       "                        0.2398,  0.0011, -0.2067, -0.0583]])),\n",
       "             ('connected2.bias',\n",
       "              tensor([ 0.0315,  0.0012,  0.0914,  0.0412, -0.1931,  0.1781, -0.0990,  0.1198])),\n",
       "             ('output.weight',\n",
       "              tensor([[-0.2659, -0.2539,  0.0986,  0.1920,  0.0350, -0.3094,  0.2818, -0.2891]])),\n",
       "             ('output.bias', tensor([0.1081]))])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegressionModel()\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x353 and 10x12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/rish/Desktop/Mini-Projects/MLProjects/torch.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rish/Desktop/Mini-Projects/MLProjects/torch.ipynb#ch0000009?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rish/Desktop/Mini-Projects/MLProjects/torch.ipynb#ch0000009?line=4'>5</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rish/Desktop/Mini-Projects/MLProjects/torch.ipynb#ch0000009?line=6'>7</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m model(X_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rish/Desktop/Mini-Projects/MLProjects/torch.ipynb#ch0000009?line=8'>9</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(y_pred, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rish/Desktop/Mini-Projects/MLProjects/torch.ipynb#ch0000009?line=10'>11</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/rish/Desktop/Mini-Projects/MLProjects/torch.ipynb Cell 8'\u001b[0m in \u001b[0;36mLinearRegressionModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rish/Desktop/Mini-Projects/MLProjects/torch.ipynb#ch0000006?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rish/Desktop/Mini-Projects/MLProjects/torch.ipynb#ch0000006?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnected1(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rish/Desktop/Mini-Projects/MLProjects/torch.ipynb#ch0000006?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnected2(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rish/Desktop/Mini-Projects/MLProjects/torch.ipynb#ch0000006?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x353 and 10x12)"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    ## testing\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model(X_test)\n",
    "\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31b54185ef9236271117b65ccd40447ed8c429f8dbf9fa6a895ee5e4cb482fd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
